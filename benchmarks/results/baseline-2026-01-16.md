# Performance baseline results (2026-01-16)

**Purpose:** Capture the current API and frontend performance baselines for trend tracking.

**Audience:** Operators, developers, and performance reviewers.

**Evidence Status:** UNKNOWN (no benchmark runs executed in this environment).

---

## Summary
These baseline entries are placeholders until Locust and Lighthouse runs are executed. Values are marked **UNKNOWN** to avoid inventing metrics.

## API performance baseline

| Metric | Target | Baseline value | Evidence | Notes |
| --- | --- | --- | --- | --- |
| API latency (p95) | < 250ms | UNKNOWN | Not measured | Run Locust and capture percentiles. |
| API latency (p99) | < 750ms | UNKNOWN | Not measured | Use the same dataset as production-like traffic. |
| API throughput (req/s) | Track trend | UNKNOWN | Not measured | Record aggregate throughput during steady-state. |

## Frontend Core Web Vitals baseline (P75)

| Metric | Target | Baseline value | Evidence | Notes |
| --- | --- | --- | --- | --- |
| LCP | < 2.5s | UNKNOWN | Not measured | Capture Lighthouse CI output (mobile + desktop). |
| INP | < 200ms | UNKNOWN | Not measured | Use Lighthouse CI trace to validate interactivity. |
| CLS | < 0.1 | UNKNOWN | Not measured | Ensure baseline uses production build. |

## How to produce measurements
1) Run Locust benchmarks per `benchmarks/README.md` with production-like data.
2) Run Lighthouse CI per `docs/OBSERVABILITY.md` from `src/frontend/`.
3) Replace **UNKNOWN** values with measured outputs and attach raw artifacts (reports, CSVs).

## Meta-commentary (for AI iteration)
- **Functionality:** This file is the canonical snapshot of baseline metrics used for trend analysis.
- **Mapping:** API metrics map to Locust results; frontend metrics map to Lighthouse CI outputs.
- **Reasoning:** Keeping the baseline in a dated file prevents overwriting history and makes regression detection auditable.

---

**Last Updated:** 2026-01-16
**Evidence Sources:** benchmarks/README.md, docs/OBSERVABILITY.md
